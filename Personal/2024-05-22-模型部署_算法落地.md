---
category: Personal
date: 2024-05-22 09:00:00 +0800
layout: post
title: 模型部署_算法落地
tag: Personal
---
## 简介

+ 与AI算法团队紧密合作，负责AI算法在CPU，GPU等处理器上的模型，并进行压缩和优化
+ 提高模型性能，降低部署成本，封装成SDK供业务端使用
+ 帮助AI算法在项目中快速落地

## 任职资格

+ 精通C++，Python，具备良好的代码书写规范和文档编写能力
+ 熟悉tensorflow等深度学习框架，熟悉CNN，RNN，LSTM等常用神经网络模型代码实现，精通网络剪枝，量化等算法优化方法
+ 熟悉ARM，X86，GPU架构体系，熟悉并行计算软件开发，具有丰富的汇编，CUDA编程经验
+ 良好的英语阅读能力，良好团队合作精神

## 经验分享 1

### 1.1 简介

+ 想要真正的将训练好的模型部署到边缘计算设备上，并能够以可接受的速度和精度来运行需要掌握很多知识。技术栈可以粗分成两个部分，即软件栈和硬件栈
  + 软件栈主要针对算法，包括对深度学习模型的了解、对模型加速技术的了解；
  + 硬件栈需要了解自己选择的边缘计算设备的硬件架构，边缘计算设备的ISP、存储、算力等性能

### 1.2 软件栈

+ 我们需要掌握三部分：模型前处理、模型推理、后处理三部分内容。

+ 前处理
  + 了解图像从获取到输入到网络需要哪些基本的操作，常见的减均值，除标准差，通道变换，RGB变换，仿射变换，透视变换等，根据业务场景不同进行组合。

+ 模型推理
  + 了解网络结构模型的设计方法和各部分算子的具体意义。对于部署模型来讲，可能不需要对模型设计有创造性的能力，但需要掌握模型结构设计的基本知识，比如一个通用的目标检测模型包含骨干网络和检测头，针对不同的应用场景，需要我们灵活地调整骨干网络，搭起一个架子，是用VGG还是Resnet，用不用FPN或者Attention模块，有必要的要加进去，没必要的别瞎往里堆。实现一个高精度的深度学习模型网络结构。计算机视觉方向有很多优秀的开源工作，有监督网络里的目标检测、实例分割、关键点检测，无监督网络里的MOCO，github上有很多star作品，比自己设计要强，能白嫖谁还自己搭是吧？

+ 后处理
  + 明白一个模型推理结束之后，如何通过后处理得到自己想要的结果，分类加sigmoid，目标检测需要nms，关键点检测需要基于heatmap映射关键点。得到基本的信息之后再拿到自己的业务场景里去做内容

### 1.3 模型加速技术基础

+ 掌握了模型推理基础内容之后，开始模型加速技术相关工作，我们需要把一个在服务器上得到的大模型变成一个小模型，方便边缘计算设备去支持、去计算。很多人直接拿着模型就去进行加速，先剪枝、蒸馏、量化来一套再说，这样不能说错，但是真正想要兼顾速度和精度，还是要深入分析一个模型的特点，再去进行针对性的加速优化

+ 分析一个模型
  + 如何分析一个模型或者如何衡量一个模型的大小？模型的大小是否单单是由运算量体现的？好多人加速完模型已测试，卧槽咋变慢了？
模型大小的衡量指标包括计算量、参数量、访存量、计算密度，前三个是绝对值，最后一个是相对值，衡量一个模型大小或者加速一个模型要从这四个方面去考量。因为内存墙的原因，访存比往往成为影响模型推理速度的一个重要因素
  + 我们不需要自己去计算这些指标，不同的平台或者后端都提供了这些指标的计算API，比如ONNX，Tensorrt，PPL，根据对应的API很快就能分析出一个模型在特定计算设备的指标情况。从模型推理角度出发，各种profiler能够更直观的看到整个模型推理过程中，流水线、计算时间等是怎么分布的。有了这些基本的认识之后，就可以针对性地进行模型加速了

+ 常用的加速技术
  + 加速技术很多，图优化技术，如结构重参数化技术；模型压缩技术如剪枝，蒸馏、量化；此外，不同的软件栈也针对性的做了很多其他方面的优化工作，kernel优化、动态内存，多线程优化技术等等，底层的技术都是类似的，只不过实现细节略有差异，性能也是千差万别。

+ 剪枝：
  + 剪去不需要的权重，相当于减少了模型的运算量，在模型的运算量是模型瓶颈的时候益处很大，各种剪枝方法也很多，设计损失函数的核心就是某个channel的权重重不重要，比如LI norm，L2 norm, Slim pruner, FPGM pruner,这些都是结构化剪枝方法，非结构化剪枝需要特定的ASIC，否则稀疏化和没做没啥区别。但当性能瓶颈不是计算量，而是访存时，剪枝技术的优点就不那么明显了。

+ 量化
  + 量化后放存量必然减少了，采用向量化的方法，计算设备单次发射的指令能够处理更多的数据，真香！
  + 主流的量化方法还是对称量化，饱不饱合的得看数据到底啥样。

+ 结构重参数化
  + 减少访存的利器，把网络结构里能合并的层合并在一起，实现访存优化，常见的就是CBR融合，RepVGG融合，关键看实操

+ 工具链
  + 大厂为我们提供了成熟的白嫖工具，自动完成所需要的优化工作：Tensorrt，NCNN轻松实现量化、结构重参，还附带其他内存优化、多线程工具，但如果你选择的边缘计算芯片不支持常用的工具链，优化工作就要自己做了，这才是真正检测功底的工作。

### 1.4 硬件栈

+ 通用计算芯片和专用计算芯片
  + 首先肯定得了解GPU，CPU，GPU代表英伟达，CPU代表intel，了解主流的GPU，CPU芯片一代代迭代，和计算相关的架构变化到底涉及了哪些内容
  + 以GPU为例，Nvidia的进化史大致是Kepler，Maxwell，Pascal，Volta，Turing, Ampere，每一代芯片都进行了那些更新，算力，显存怎么做的提升，都支持了那些操作等等

+ 专用芯片硬件架构
  + CPU、GPU架构主要在任务通用性上下了功夫，相较于专门针对计算机视觉的专用芯片性能很大差距。专用的ASIC种类很多了，架构各有特色，但都是为了一个目的：高性能+低功耗。以Google为代表的TPU问世后掀起了专用ASIC的研发热潮，这十年国产芯片也在疯狂输出，FPGA、KPU，NPU等各种PU层出不穷，性能鱼龙混杂，我们还是要擦亮眼睛去识别的
  + 涉及到模型部署还是要优选搭配软件栈的芯片，自己去实测芯片性能是最可靠的方法，别看彩页说的如何牛p，还得看模型、看实测！
了解视觉信号从输入芯片到输出芯片全流程参与的硬件，这些硬件是怎么参与整个模型的前处理、推理和后处理的，比如昇腾的芯片采用专用硬件实现img2col，Nvidia采用软件实现img2col，各自的特点是怎么样的；再比如Nvidia专用的硬件解码，对于模型推理有怎样的增益等等

### 1.5 编程语言

+ Python是一门高级抽象语言，便利了深度学习模型搭建、训练，但用来部署无法和C++一较高下了，

+ 简单说说针对算法部署需要掌握的C++知识有哪些？
  + 面向对象编程的基础：类
  + 继承：构造函数、析构函数、指针、引用、重写，模板函数
  + STL：常用的序列容器及特性，共享内存
  + C++多线程：掌握 threading, mutex,condition_variable,future，RAII怎么用
  + C++动态内存管理：我也不会，google去吧
  + 编译基础，动静态库，makefile

## 经验分享 2 

+ 一般部署的每一个硬件平台都有自己的推理框架，需要自己做的任务基本上是调用api来对一些模型进行落地，这些其实并不是很难
+ 如果想进阶，学习一些优化的知识，例如卷积优化算法，或者更深入的汇编优化等等，可以关注知乎移动端专栏或者GiantPandaCV公众号，都会带来一些比较良心的部署优化相关的文章

## 经验分享 3

+ 这个工作是开发，对算法的熟悉是为了帮助工程实现，对算法的要求和研究员的关键区别是不太需要算法上的创新

## 参考资料

+ https://www.zhihu.com/question/411393222