## 简介

+ 模型部署相关笔记
+ 模型部署需要的技术栈还是比较复杂的，不仅需要底层并行知识，还需要模型算法知识，不过对于两者的要求都不是很高，只需要能够看的懂最新进展，能够跟着复现就行，模型部署，最重要的还是工程能力

## 模型有几种形式

+ 模型的形式有很多种，模型部署说到底还是编程任务，使用代码来实现一套函数，只不过这个函数中的参数都被提取出来封装到了一起，称为模型。
+ 也正是因为如此，不同的框架不同的代码生成的模型格式五花八门，从Pytorch到tf到mxnet等训练框架，它们保存的模型各不相同，我们首先要解决的就是统一模型的格式，这里的统一格式并不是指文件的后缀一样就可以了，而是指所要支持的算子要统一。

## 模型部署是什么

+ 模型部署是指将训练好的机器学习模型部署到生产环境中，以便实际应用中使用。在模型发布之前，需要将模型从训练环境中导出，然后将其部署到生产环境中，通常是作为一个服务或者一个库的形式。

## 如何模型部署

+ 模型部署的过程通常包括以下几个步骤
  + 导出模型: 将训练好的模型导出为可部署的格式，例如PMML， ONNX， TensorFlow等
  + 部署模型: 将导出的模型部署到生产环境中，通常是作为一个服务或者一个库的形式。
  + 测试模型: 在生产环境中对模型进行测试，以确保其能够正常工作，并且输出结果符合预期
  + 监控模型: 在生产环境中对模型进行监控，以便及时发现并解决问题
+ 模型发布是机器学习应用中重要的一环，它可以帮助企业快速将机器学习应用落地，并带来实际的商业价值。

+ 模型的部署方式可以根据具体的应用场景和需求而定，以下是一些常见的模型部署方式
  + WebAPI: 将模型部署为一个Web服务，通过HTTP请求来获取模型预测的结果。可以使用Flask,Django等
  + 嵌入式设备: 将模型部署到嵌入式设备上，例如数梅派，Jetson Nano等。可以使用TensorFlow Lit, Pytorch Mobile等框架来实现
  + 容器化: 将模型打包成docker镜像，然后部署到云服务器上。可以使用Kubernetes, Docker Swarm等容器编排工具来实现
  + 边缘计算: 将模型部署到边缘设备上，例如智能摄像头，智能家居等。可以使用TensorFlow.js, TensorFlow Lite等框架来实现。
+ 无论采用哪种部署方式，都需要考虑模型的性能，安全性，可靠性，可维护性等方面的问题。

## 模型部署的应用场景

+ 图像识别：将图像识别模型部署到智能摄像头、安防监控系统等设备上，实现人脸识别、车牌识别等功能。
+ 自然语言处理：将自然语言处理模型部署到智能客服、聊天机器人等应用中，实现自然语言理解、情感分析等功能。
+ 推荐系统：将推荐系统模型部署到电商、社交网络等应用中，实现商品推荐、好友推荐等功能。
+ 工业控制：将模型部署到工业生产线上，实现质量检测、故障预测等功能。
+ 金融风控：将模型部署到银行、保险等金融机构中，实现风险评估、欺诈检测等功能。
+ 医疗诊断：将模型部署到医疗设备中，如医学影像诊断、病理分析等应用中，实现疾病诊断、治疗方案推荐等功能。
+ 智能交通：将模型部署到交通系统中，实现交通流量预测、智能路灯等功能。
+ 物联网：将模型部署到物联网设备中，如智能家居、智能城市等应用中，实现环境监测、交通管理等功能。
+ 游戏开发：将模型部署到游戏中，实现智能 NPC、游戏推荐等功能。

## 模型部署的注意事项

+ 确保模型的正确性：在部署模型之前，需要对模型进行充分的测试，以确保其能够正确地工作，并且输出结果符合预期。
+ 选择适合的部署方式：不同的应用场景需要不同的部署方式，需要根据具体的需求选择适合的部署方式。
+ 考虑性能和资源消耗：在部署模型时需要考虑模型的性能和资源消耗，以确保模型能够在生产环境中高效地运行。
+ 考虑安全性和隐私保护：在部署模型时需要考虑安全性和隐私保护，以确保模型不会被恶意攻击或者泄露用户隐私。
+ 建立监控和反馈机制：在部署模型后，需要建立监控和反馈机制，及时发现并解决模型出现的问题，以确保模型能够持续地稳定运行。

## 模型部署模型的实现类库

+ Python中有很多方法可以实现模型部署，以下是一些常见的方法：
  + Flask：Flask是一个轻量级的Web框架，可以用来搭建Web API。通过Flask，可以将模型部署为一个Web服务，通过HTTP请求来获取模型预测结果。
  + Django：Django是一个功能强大的Web框架，可以用来搭建Web应用程序。通过Django，可以将模型部署为一个Web应用程序，实现更复杂的业务逻辑。
  + FastAPI：FastAPI是一个高性能的Web框架，可以用来搭建Web API。与Flask相比，FastAPI具有更高的性能和更好的类型注释支持。
  + TensorFlow Serving：TensorFlow Serving是一个专门用于模型部署的框架，可以快速部署TensorFlow模型，并提供高性能的预测服务。
  + ONNX Runtime：ONNX Runtime是一个高性能的推理引擎，可以用于部署ONNX格式的模型。ONNX Runtime支持多种硬件平台和操作系统，包括CPU、GPU、FPGA等。
  + PyTorch Serving：PyTorch Serving是一个专门用于PyTorch模型部署的框架，可以快速部署PyTorch模型，并提供高性能的预测服务。
  + AWS Lambda：AWS Lambda是亚马逊云提供的一种无服务器计算服务，可以用来运行代码。通过AWS Lambda，可以将模型部署为一个无服务器应用程序，实现高可用性和低成本。

## 模型部署后的性能评价

+ 模型部署后，我们需要对模型的性能进行评价，以确保模型能够在生产环境中高效地运行。以下是一些常见的模型性能评价方法：
  + 响应时间：响应时间是指从接收请求到返回结果所需的时间。在模型部署后，我们需要对模型的响应时间进行评价，以确保模型能够在实时应用中快速响应。
  + 吞吐量：吞吐量是指在单位时间内处理的请求数量。在模型部署后，我们需要对模型的吞吐量进行评价，以确保模型能够在高并发场景下处理大量请求。
  + 准确率：准确率是指模型在测试集上的分类准确率。在模型部署后，我们需要对模型的准确率进行评价，以确保模型在生产环境中能够保持良好的预测性能。
  + 内存占用：内存占用是指模型在运行时所占用的内存大小。在模型部署后，我们需要对模型的内存占用进行评价，以确保模型能够在生产环境中高效地利用资源。
  + CPU和GPU利用率：CPU和GPU利用率是指在模型运行时，CPU和GPU的利用率。在模型部署后，我们需要对CPU和GPU的利用率进行评价，以确保模型能够充分利用硬件资源。

## Pytorch

+ 保存为pt文件或者pth文件，pytorch导出的模型有两种方案
  + 第一种是不仅仅包含参数，还包含了模型结构，读取的时候不需要预先建立模型
  + 第二种是仅仅包含参数，读取之前需要将模型建立好，通过pth文件往里面填参数。
+ 但是第一种方案也不是在任何地方都可以直接使用，还是要有模型定义代码，也就是说这两种模型都只能由pytorch使用。

+ pytorch还支持另一种导出
  + 导出成torchscript格式，这种确确实实是将模型结构和参数都保存了，不仅可以用在pytorch，还可以用在C++推理上，不过C++推理依旧需要torchlib，相当于还是离不开torch

## ONNX

+ onnx是一套开放的中间格式标准，目前大部分深度学习相关的工具都支持onnx。onnx与训练框架，推理框架都无关
+ onnx模型由三部分组成
  + 节点Node: 就是神经网络中一层
  + 输入Input: 存储了输入矩阵的维度信息
  + 初始化器Initializer: 存储了权重和参数
+ 三种之间互关联，相互依赖，很难修改

+ 在编译器领域，为了支持不同的编程语言和不同的运行平台，提出了一种与编程语言和运行平台无关的中间表达语言称为IR，onnx也可以看作是深度学习领域的IR

## 推理系统

+ 有了通用的模型格式，深度学习要融入到整个产品中需要设计成一个单独的系统，响应处理外界请求，这就是推理系统的工作。
+ 推理系统需要考虑包括模型管理，服务接口设计，系统检测，系统调度等。推理系统更多的是将推理工作包装成一个服务，供外界使用，推理系统需要考虑的事情
  + 吞吐量
  + 响应效率
  + 扩展性
  + 灵活性

## 推理引擎

+ 推理引擎主要做的是： 优化模型，实现核心算子，开发目标平台调度引擎。
+ 本来像OpenVINO和TensorRT这种是厂家做出来的，支持的平台也只有它们自己的平台。
+ 随着深度学习大火，各种推理框架都发展了起来。