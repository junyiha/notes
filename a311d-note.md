
# 模型转换

模型转换简介:
+ 模型转换过程是:先将模型量化成int8, int16或者uint8数据格式,再转换成基于平台运行的nbg格式文件,并导出运行的case代码.

量化简介:
+ 量化是将以往用32bit或者64bit表达的浮点数用16bit,8bit或者更低的2bit方式进行存储

量化方式选取规则:
+ 通常情况下,选择uint8/int8的方式均可.uint8是google推荐量化方式
+ 一般建议不选取int16,相较8bit量化,模型大一倍,并且在精度上没有明显的优势
+ 参考模型前处理后的取值范围选择量化方式.比如,如果模型前处理后的取值范围为(0~255),此时建议直接选取u8的量化方式
+ 先随便选择一种量化方式,执行完量化操作后,查看生成的.quantize文件,确认统计的max_value/min_value,如果出现绝对值大于256的情况,建议选取int16方式量化,如果绝对值出现大于128的情况,建议选取u8量化方式,其他情况选择int8/u8均可

参数:
+ `--channel-mean-value` : 根据训练模型时预处理方式来设置该预处理的命令行参数.包括四个值(m1, m2, m3, scale). 
  + 前三个值为均值参数,最后一个值为scale参数.
  + 如果前处理需要将数据归一化到[-1, 1]之间,参数设置为(128 128 128 0.0078125)
  + 如果前处理需要将参数归一化到[0, 1]之间,参数设置为(0 0 0 0.00392156)
  + 如果是单通道参数,设置方式为(m1, 0, 0, scale)
  + 最后一个数scale为小数,比如:0.0078125(1/128), 0.00392156(1/256)
+ `--source-file dataset.txt`:中是给定的量化输入图片的路径.建议提供200张左右,同时也是模型使用运行场景的图片来进行量化,确保统计出的最大/最小值为实际运行场景的最大/最小值范围,这样量化效果会更佳
+ `--quantizer 和 --qtype`一起用于选择量化类型,支持的类型为:
  + `asymmetric_affine`  : 对应`qtype`选择设置`uint8`
  + `dynamic_fixed_point`: 对应`qtype`选择设置`int8`或者`int16`
  + `perchannel_symmetric_affine`:perchannel量化,对应`qtype`设置`int8`,当前只有E8对应芯片支持(当前A311D的板子是DOX88)
+ `--rebuild`:默认参数,建议携带.在前后使用不同的量化方式进行量化时,如果没有手动删除已生成的.quantize文件,在没有携带该参数时,第二次量化未做任何处理
+ `--batch-size`和`--iterations`:为多张图片量化时设置参数.如果图片数量为>1,需添加量化参数`--batch-size(默认为1)`和`--iterations(默认为1)`.
  + `iterations * batch_size = 图片数量`,例如:5000张图片,设置参数为:`--batch-size 100 --iterations 50
+ `--optimize`:当前板子的版本对应`VIPNANOQI_PID0X88`
+ `--viv-sdk`:依赖的sdk包,acuity_tool_xxx/bin目录下vcmdtools目录
+ `--pack-nbg-unify`:生成nbg文件